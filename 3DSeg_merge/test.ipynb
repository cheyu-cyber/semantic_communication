{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37922810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import vpi\n",
    "\n",
    "# Paths to the stereo pair (left/right)\n",
    "left_path = Path(\"000042_10_L.png\")\n",
    "right_path = Path(\"000042_10_R.png\")\n",
    "\n",
    "if not left_path.exists() or not right_path.exists():\n",
    "    raise FileNotFoundError(\"Expected stereo pair 000042_10_[L|R].png in the current directory.\")\n",
    "\n",
    "# Load images as grayscale for stereo matching\n",
    "left_img = cv2.imread(str(left_path), cv2.IMREAD_GRAYSCALE)\n",
    "right_img = cv2.imread(str(right_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "if left_img is None or right_img is None:\n",
    "    raise RuntimeError(\"Failed to read stereo images with OpenCV.\")\n",
    "\n",
    "# Choose GPU backend when available, otherwise fall back to CPU\n",
    "\n",
    "with vpi.Backend.CUDA:\n",
    "    left_vpi = vpi.asimage(left_img)\n",
    "    right_vpi = vpi.asimage(right_img)\n",
    "    disparity_vpi = vpi.stereodisp(left_vpi, right_vpi, window=5, maxdisp=128)\n",
    "    disparity = disparity_vpi.cpu().astype(np.float32)\n",
    "\n",
    "# Normalize disparity for visualization\n",
    "disp_norm = cv2.normalize(disparity, None, 0, 255, cv2.NORM_MINMAX)\n",
    "disp_color = cv2.applyColorMap(disp_norm.astype(np.uint8), cv2.COLORMAP_JET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8cd380",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from efficientvit.seg_model_zoo import create_efficientvit_seg_model\n",
    "from eval_efficientvit_seg_model_copy import CityscapesDataset, Resize, ToTensor, get_canvas\n",
    "from typing import Any, Optional\n",
    "# ---------------------------------------------------------\n",
    "# 1. SETUP & CLASS DEFINITIONS\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Cityscapes 19-class palette (Standard)\n",
    "CITYSCAPES_CLASSES = {\n",
    "    0: 'road', 1: 'sidewalk', 2: 'building', 3: 'wall', 4: 'fence',\n",
    "    5: 'pole', 6: 'traffic light', 7: 'traffic sign', 8: 'vegetation',\n",
    "    9: 'terrain', 10: 'sky', 11: 'person', 12: 'rider', 13: 'car',\n",
    "    14: 'truck', 15: 'bus', 16: 'train', 17: 'motorcycle', 18: 'bicycle'\n",
    "}\n",
    "\n",
    "# Colors for visualization (Randomized for distinction)\n",
    "np.random.seed(42)\n",
    "COLORS = np.random.randint(0, 255, size=(20, 3), dtype=np.uint8)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. MODEL LOADING\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# A. Load YOLO11-seg (Foreground Specialist)\n",
    "print(\"Loading YOLO11-seg...\")\n",
    "yolo_model = YOLO(\"yolo11n-seg.pt\")  # using 'nano' for speed\n",
    "yolo_model.to(\"cpu\")\n",
    "# B. Load EfficientViT-Seg (Background Specialist)\n",
    "print(\"Loading EfficientViT-Seg...\")\n",
    "\n",
    "\n",
    "# We use 'b0' (fastest) trained on 'cityscapes'\n",
    "eff_model = create_efficientvit_seg_model(name=\"efficientvit-seg-b0-cityscapes\", pretrained=True).cuda()\n",
    "eff_model.eval()\n",
    "\n",
    "# Move to GPU if available\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"Using device: {device}\")\n",
    "# eff_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfbaf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 3. HELPER FUNCTIONS\n",
    "# ---------------------------------------------------------\n",
    "def resize(\n",
    "    x: torch.Tensor,\n",
    "    size: Optional[Any] = None,\n",
    "    scale_factor: Optional[list[float]] = None,\n",
    "    mode: str = \"bicubic\",\n",
    "    align_corners: Optional[bool] = False,\n",
    ") -> torch.Tensor:\n",
    "    if mode in {\"bilinear\", \"bicubic\"}:\n",
    "        return torch.nn.functional.interpolate(\n",
    "            x,\n",
    "            size=size,\n",
    "            scale_factor=scale_factor,\n",
    "            mode=mode,\n",
    "            align_corners=align_corners,\n",
    "        )\n",
    "    elif mode in {\"nearest\", \"area\"}:\n",
    "        return torch.nn.functional.interpolate(x, size=size, scale_factor=scale_factor, mode=mode)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"resize(mode={mode}) not implemented.\")\n",
    "\n",
    "def preprocess_for_efficientvit(image_path):\n",
    "    \"\"\"Resizes and normalizes image for EfficientViT\"\"\"\n",
    "    img = np.array(Image.open(image_path).convert('RGB'))\n",
    "    # EfficientViT expects specific preprocessing (Standard ImageNet norm)\n",
    "    transform = transforms.Compose([\n",
    "        Resize((1024, 2048)), # Standard Cityscapes resolution\n",
    "        ToTensor(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform({\"data\": img, \"label\": np.ones_like(img)})[\"data\"], img\n",
    "\n",
    "def get_contours_from_mask(binary_mask):\n",
    "    \"\"\"Extracts contour points from a binary mask\"\"\"\n",
    "    # cv2.findContours expects uint8 single channel\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        # Return the largest contour found (main object body)\n",
    "        c = max(contours, key=cv2.contourArea)\n",
    "        return c.reshape(-1, 2).tolist() # Convert [[x,y]] format to list of [x,y]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc536b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 4. MAIN PIPELINE\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def run_hybrid_segmentation(image_path):\n",
    "    final_objects = []\n",
    "    \n",
    "    # --- Step 1: Run EfficientViT (Background) ---\n",
    "    input_tensor, original_img_pil = preprocess_for_efficientvit(image_path)\n",
    "    class_colors = CityscapesDataset.class_colors\n",
    "    orig_h, orig_w = original_img_pil.shape[:2]\n",
    "\n",
    "    model_device = next(eff_model.parameters()).device\n",
    "    with torch.inference_mode():\n",
    "        batch_tensor = input_tensor.unsqueeze(0).to(model_device, non_blocking=True)\n",
    "        eff_output = eff_model(batch_tensor)\n",
    "        if eff_output.shape[-2:] != (orig_h, orig_w):\n",
    "            eff_output = resize(eff_output, size=(orig_h, orig_w))\n",
    "        semantic_map = eff_output.argmax(dim=1).squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "\n",
    "    canvas = get_canvas(original_img_pil, semantic_map, class_colors)\n",
    "    Image.fromarray(canvas).save(\"./test_results/efficientvit_output.png\")\n",
    "\n",
    "    del eff_output\n",
    "    del batch_tensor\n",
    "    del input_tensor\n",
    "    if model_device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Extract \"Stuff\" (Road, Sky, Building) from Semantic Map\n",
    "    # We only care about specific background classes for the hybrid model\n",
    "    stuff_classes_of_interest = [0, 10, 2] # 0=Road, 10=Sky, 2=Building\n",
    "    \n",
    "    for cls_id in stuff_classes_of_interest:\n",
    "        # Create binary mask for this class\n",
    "        mask = (semantic_map == cls_id).astype(np.uint8) * 255\n",
    "        \n",
    "        # Filter: Only keep if it's a significant chunk of the image\n",
    "        if np.sum(mask) > (orig_h * orig_w * 0.01): \n",
    "            contours = get_contours_from_mask(mask)\n",
    "            if contours:\n",
    "                final_objects.append({\n",
    "                    \"name\": CITYSCAPES_CLASSES[cls_id],\n",
    "                    \"index\": int(cls_id),\n",
    "                    \"contour\": contours,\n",
    "                    \"box\": cv2.boundingRect(np.array(contours)) # x,y,w,h\n",
    "                })\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    # --- Step 2: Run YOLO11 (Foreground) ---\n",
    "    # YOLO handles resizing internally, so we pass the path or raw array\n",
    "    yolo_results = yolo_model(image_path, verbose=False)[0].cpu()\n",
    "\n",
    "    if yolo_results.masks is not None:\n",
    "        for i, mask_data in enumerate(yolo_results.masks.data):\n",
    "            # YOLO masks are float tensors, usually smaller size\n",
    "            mask_np = mask_data.numpy()\n",
    "            \n",
    "            # Resize mask to original image size\n",
    "            mask_resized = cv2.resize(mask_np, (orig_w, orig_h))\n",
    "            binary_mask = (mask_resized > 0.5).astype(np.uint8) * 255\n",
    "            \n",
    "            # Get Class Name\n",
    "            class_id = int(yolo_results.boxes.cls[i].item())\n",
    "            class_name = yolo_model.names[class_id]\n",
    "            \n",
    "            contours = get_contours_from_mask(binary_mask)\n",
    "            \n",
    "            if contours:\n",
    "                final_objects.append({\n",
    "                    \"name\": class_name,\n",
    "                    \"index\": 100 + i, # Offset index to differentiate from semantic IDs\n",
    "                    \"contour\": contours,\n",
    "                    \"box\": yolo_results.boxes.xywh[i].numpy().tolist()\n",
    "                })\n",
    "\n",
    "    del yolo_results\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return original_img_pil, final_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16886582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 5. VISUALIZATION\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Download a sample street image000042_10_L.png\n",
    "\n",
    "image, objects = run_hybrid_segmentation(\"000042_10_L.png\")\n",
    "\n",
    "# Draw Results\n",
    "vis_img = image.copy()\n",
    "\n",
    "for obj in objects:\n",
    "    pts = np.array(obj['contour'], np.int32)\n",
    "    pts = pts.reshape((-1, 1, 2))\n",
    "    \n",
    "    # Pick a color (Green for YOLO objects, Blueish for Background)\n",
    "    if obj['index'] < 100: # Background\n",
    "        color = (255, 0, 0) # Blue\n",
    "    else: # Foreground\n",
    "        color = (0, 255, 0) # Green\n",
    "        \n",
    "    # Draw Contour\n",
    "    cv2.polylines(vis_img, [pts], isClosed=True, color=color, thickness=3)\n",
    "    \n",
    "    # Draw Label\n",
    "    box = obj['box']\n",
    "    x, y = int(box[0]), int(box[1])\n",
    "    cv2.putText(vis_img, f\"{obj['name']}\", (x, y - 10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "# Show Image\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(vis_img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Print Data Output (First 3 objects as example)\n",
    "print(\"--- Extracted Object Data (Preview) ---\")\n",
    "for i, obj in enumerate(objects[:3]):\n",
    "    print(f\"Object {i+1}: Name='{obj['name']}', Index={obj['index']}, Box={obj['box']}\")\n",
    "    print(f\"Contour (First 5 pts): {obj['contour'][:5]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
